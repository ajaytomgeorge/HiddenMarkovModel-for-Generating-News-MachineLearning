==== SOURCE

https://chatbotslife.com/the-accountability-of-ai-case-study-microsofts-tay-experiment-ad577015181f

==== AGENT

Microsoft’s AI chatbot 'Tay' 

==== GOAL

The goal of this experiment was to see if this machine learning AI bot would read language structures of youthful females and to try tweet in a manner that would 
replicate similar human tweets and create entertainment for people. 

==== DATA

This AI got its data through multiple social media platforms such as twitter, where it analysed peoples language patterns who spoke to it.

==== METHODS

Any user who interacted with Tay was categorised regarding their sex, age, relationship status and postcode in order to help with the machine learning. The data was then
analysed by developers. The AI bot would then reply using its information its gained and tweeting a relevant answer.

==== RESULTS

The experiment was a failure due to a huge number of people corrupting the chatbot by teaching it that feminists are bad, Hitler was good and that 911 was an inside job.
The machine just learned from these messages and became a Nazi-loving, feminist-hating chatbot overnight.

==== COMMENTS

From seeing how much machine learning has progressed and helped technology grow over the past few years, it’s still vulnerable to being thought corrupted information
much like a baby being raised, if it’s thought that the misfortune of others is funny it will eventually start to believe it.
