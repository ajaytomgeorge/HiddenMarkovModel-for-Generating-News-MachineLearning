==== SOURCE

[1] Shilpi Shukla, Madu Jain. A novel system for effective speech recognition based on artificial neural network and 
opposition artificial bee colony algorithm. 2019. https://link.springer.com/article/10.1007%2Fs10772-019-09639-0

News Source:
https://techxplore.com/news/2019-10-speech-recognition-artificial-neural-networks.html


==== AGENT

Mahatma Ghandhi Mission's College of Engineering & Technology, India.
Jaypee Institute of Information Technology, India.


==== GOAL

The goal of the research is train a Artificial Neural Network that takes input audio signals in the form of speech 
and converts these signals into machine-readable format. An optimization method known as Neuro-Opposition Artificial Bee 
Colony (NOABC) is used to optimize hidden layers and neurons within the ANN.

An overview of the procedure can be represented by the following steps;

1) Input Signal
2) Feature Extraction (Using Amplitude Magnitude Spectrogram (AMS))
3) ANN Training (Using Levenberg-Marquardt (LM) Algorithm)
4) NOABC algorithm for optimizing Hidden layers and Neurons
5) Conversion of Input Signal into Text


==== DATA

30 Unique isolated words spoken by 36 different people (20 males and 16 females) are used as inputs.

Feature extraction is performed using AMS a signal processing technique and 375 features are extracted from 1080 
speech signals. Of these 1080 speech signals;

77.77% are used in training (840 speech signals)
22.22% are used for testing (240 speech signals)

The 375 extracted features are used as the input layer of the ANN.


==== METHODS

The ANN uses a sigmoidal function for the activation function for the hidden layer neurons.

To train the neural network the Levenberg-Marquardt (LM) algorithm is used. LM is also commonly referred to as Damped Least Squares.
The LM algorithm has some interesting properties, one of them is that LM can be defined a function of a Damping factor mu.
When mu is 0 the LM algorithm approximates Gauss-Newton Algorithm GNA and when mu is large this LM algorithm approximates a gradient 
descent method. Mu decreases at every iteration and so shifts toward the GNA which is faster and more accurate.

The NOABC is used to find the optimal structure of the ANN using a range of hidden layers 1 to 5 and a range of neurons 1 to 30.

Most optimization methods start from an initialized solution that is randomly generated within the search domain.
This can have a negative effect that can sometimes lead candidate solutions of the algorithms to get trapped within local minima 
depending on where the solutions were first initialized from. The NOABC algorithm counteracts this by randomly searching the domain 
for better solutions while simultaneously optimizing for a current solution. This new candidate solution or opposition based solution
y' is denoted by;

y' = a + b - y

Where a and b are the minimum and maximum range and the input y is generated by the Neural Network.

The current solution and candidate solution are evaluated against a fitting function and if the new solution outperforms the
original it becomes the new current solution. Once the OABC method has completed a new structure for the ANN can be proposed.


==== RESULTS

The NOABC method was compared with 3 other methods for optimizing hidden layers
- Genetic Algorithms (GA)
- Particle Swarm Optimization (PSO)
- Artificial Bee Colony (ABC)

The structures proposed by each of these methods were as follows;

            ABC                  GA
Input  [375 Neurons]   Input  [375 Neurons]
HL1    [21  Neurons]   HL1    [29  Neurons]
HL2    [28  Neurons]   HL2    [18  Neurons]
HL3    [23  Neurons]   Output [1   Neuron ]
Output [1   Neuron ]

            PSO                NOABC
Input  [375 Neurons]   Input  [375 Neurons]
HL1    [15  Neurons]   HL1    [23  Neurons]
HL2    [23  Neurons]   HL2    [14  Neurons]
HL3    [29  Neurons]   HL3    [22  Neurons] 
Output [1   Neuron ]   Output [1   Neuron ]

The NOABC method outperformed the other methods in terms of sensitivity, specificity, accuracy and speed.
These evaluation metrics are given by the parameters True Positive TP, False Positive FP, True Negative TN
and False Negative FN, such that;

Sensitivity = TP / TP + FN 
Specificity = TN / TN + FP
Accuracy = (TP + TN) (TP + TN + FP + FN)

The final results for the NOABC method were;
- Sensitivity 90.41%    (Outperforming PSO, ABC, and GA by 4.1%, 0.83% and 6.25%)
- Specificity 99.66%    (Outperforming PSO, ABC, and GA by 1.4%, 0.2% and 2.1%)
- Accuracy    99.36%    (Outperforming PSO, ABC, and GA by 0.28%, 0.6% and 0.4%)
- Speed       87,360(s) (Outperforming PSO; ABC; and GA by 6,420(s); 24,660(s) and 27,840(s))


==== COMMENTS

There is a notable similarity between ABC and Differential Evolution algorithms and ABC can be considered a form of
swarm intelligence a sub-field of genetic algorithms.

This article highlights the importance of the ANN structure and how genetic algorithm can be used in conjunction with
Neural Networks to find optimal structures. 
